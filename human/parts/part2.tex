\part{The Black Box Dilemma and the Rise of Explainable AI (XAI)}

\section{The ``Black Box'' Problem in Deep Learning}

The remarkable success of Deep Learning models in emulating complex cognitive functions \cite{lecun2015deep}, as explored in Section~\ref{sec:2000s}, brings with it a significant challenge: their inherent opacity. Deep neural networks, with their vast number of parameters and intricate, multi-layered architectures, often operate as ``black boxes'' \cite{doshi2017towards}. While their input-output behavior can be meticulously evaluated, the internal computations and learned representations that lead to a specific decision or prediction are frequently obscure and difficult for humans to comprehend directly. This lack of transparency means that even when a model achieves high performance, understanding \textit{why} it performs well or \textit{how} it arrives at its conclusions can be an arduous task, undermining trust and hindering deeper scientific inquiry \cite{ribeiro2016why, miller2019explanation}.

This ``black box'' characteristic poses a particular impediment when these models are applied to Neuroscience, where the primary objective extends beyond mere replication of brain-like outputs to a fundamental understanding of the underlying neural mechanisms \cite{richards2019deep, savage2019how}. If a Deep Learning model accurately predicts neural responses in the visual cortex \cite{yamins2016using} or simulates language processing \cite{caucheteux2022brains} but its internal strategies remain inscrutable, it offers limited insight into how the biological brain actually performs these tasks. The aspiration of cognitive computational Neuroscience is to develop models that are not only predictive but also explanatory, providing testable hypotheses about brain function \cite{kriegeskorte2018cognitive}. A model that functions as an uninterpretable oracle, however accurate, falls short of this goal. It may demonstrate that a certain mapping from stimulus to neural activity or behavior is learnable, but it fails to reveal the computational principles or representational transformations that the brain itself might employ. Consequently, the scientific value for reverse-engineering brain mechanisms is obscured, as it becomes challenging to validate whether the model's learned solutions are neurobiologically plausible or to derive new insights into the brain's algorithms. The critical need, therefore, is to develop approaches that can illuminate these internal workings, transforming powerful predictive tools into genuinely explanatory frameworks for understanding the brain.

\section{A Taxonomy of Explainable AI Methods}\label{sec:XAI-tax}

To address the opacity of Deep Learning models, the field of Explainable AI (XAI) has emerged, offering a diverse variety of methods to take a look inside the ``black box'' \cite{doshi2017towards, miller2019explanation}. A primary distinction in XAI methodologies lies between \textit{post-hoc interpretability techniques}, which are applied to already trained models, and \textit{intrinsically explainable models}, which are designed for transparency from the ground up. While some models like linear regression or decision trees are inherently interpretable due to their simple structure, the complex, highly non-linear nature of deep neural networks means they seldom fall into this category. Consequently, much of XAI research focuses on post-hoc methods to analyze these powerful but opaque systems \cite{doshi2017towards}.

One prominent category of post-hoc XAI techniques comprises \textbf{Feature Attribution Methods}. These methods aim to identify which parts of the input data a model deems most important when making a specific prediction.
\begin{itemize}
    \item \textbf{Saliency Maps} are a common feature attribution technique, particularly in computer vision. They generate heatmaps that highlight the pixels in an input image that most significantly influence the model's output for a given class \cite{fong2017interpretable}. Typically, these are computed by examining the gradient of the output prediction score with respect to the input pixel values. For neuroscientists, saliency maps can offer a visual hypothesis about which features a model, trained to mimic a sensory processing task, might be using, analogous to how one might study receptive fields in biological neurons.
    \item \textbf{LIME (Local Interpretable Model-agnostic Explanations)} and \textbf{SHAP (SHapley Additive exPlanations)} offer more sophisticated approaches to feature attribution. LIME operates by training a simpler, interpretable model (e.g., a linear model or a decision tree) to approximate the behavior of the complex black-box model in the local vicinity of a particular instance being explained \cite{ribeiro2016why}. This provides a local, understandable rationale for a specific prediction without requiring insight into the global structure of the original model. SHAP, on the other hand, draws from cooperative game theory, specifically Shapley values, to assign a unique contribution value to each feature for a given prediction, providing a unified and theoretically grounded framework for feature importance \cite{lundberg2017unified}. These methods are valuable because they can be applied to virtually any model (model-agnostic) and help study the reasoning behind individual predictions, which is crucial when trying to understand if an AI model's strategy bears any resemblance to neural computation.
\end{itemize}

Beyond attributing importance to input features, another class of XAI techniques focuses on \textbf{Model-based Interpretation}, aiming to understand the internal mechanisms and learned representations within the network itself.
\begin{itemize}
    \item \textbf{Analyzing Network Activations and Representations} involves directly inspecting the internal state of the network. This can include visualizing the activation patterns of individual neurons or entire layers in response to different inputs, or using techniques like feature visualization, which attempts to synthesize an input that maximally activates a specific neuron or feature detector within the network \cite{olah2018building}. Such methods allow researchers to probe what kinds of information are encoded at different stages of processing. For instance, the hierarchical features that CNNs learn when trained on image recognition, as we mentioned in Section~\ref{sec:2000s}~\cite{yamins2016using}. Other tools, like Network Dissection~\cite{bau2017network}, aim to quantify the interpretability of individual units by identifying the semantic concepts (e.g., 'door', 'sky') that they detect.

    \item \textbf{Ablation Studies} in artificial neural networks are directly inspired by lesion studies in Neuroscience. These methods involve systematically perturbing the model (for example, by desactivating specific artificial neurons, connections, or entire layers) and then observing the impact on the model's behavior or internal representations \cite{kriegeskorte2018cognitive}. If silencing a particular component significantly degrades performance on a specific task, it suggests that this component plays a crutial role in that computation. Such ``in silico lesioning'' can help to map functional roles to different parts of the network, providing testable hypotheses about the functional specialization of the different regions of the network.
\end{itemize}
These various XAI approaches are not mutually exclusive and are often used in combination to build a more comprehensive understanding of how complex AI models operate, thereby offering a richer set of tools for comparison with, and generation of hypotheses about, the brain.

\section{Explainable AI in Action: Bridging the Gap to Neuroscience}
\begin{itemize}
    \item \textbf{Reverse-Engineering Sensory Systems:}
        \begin{itemize}
            \item \textbf{Vision:} Beyond object recognition, how XAI helps us understand how CNNs represent texture, shape, and motion, and how this compares to neural data from different visual areas.
            \item \textbf{Audition:} Using Deep Learning models to understand the neural coding of sound, from the cochlea to the auditory cortex.
        \end{itemize}
    \item \textbf{Decoding and Encoding Models:} Using explainable models to predict neural activity from stimuli (encoding) and to decode mental states or perceived stimuli from neural recordings (decoding).
    \item \textbf{Understanding Higher Cognitive Functions:}
        \begin{itemize}
            \item \textbf{Language:} Analyzing the representations within large language models (LLMs) like BERT and GPT, and comparing them to the brain's language processing centers (e.g., Broca's and Wernicke's areas).
            \item \textbf{Decision-Making and Reinforcement Learning:} How explainable reinforcement learning models can shed light on the neural circuits involved in reward, planning, and action selection.
        \end{itemize}
\end{itemize}

\clearpage