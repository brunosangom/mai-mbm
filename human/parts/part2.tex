\part{The Black Box Dilemma and the Rise of Explainable AI (XAI)}

\section{The ``Black Box'' Problem in Deep Learning}

The remarkable success of Deep Learning models in emulating complex cognitive functions \cite{lecun2015deep}, as explored in Section~\ref{sec:2000s}, brings with it a significant challenge: their inherent opacity. Deep neural networks, with their vast number of parameters and intricate, multi-layered architectures, often operate as \textbf{black boxes} \cite{doshi2017towards}. While their input-output behavior can be meticulously evaluated, the internal computations and learned representations that lead to a specific decision or prediction are frequently obscure and difficult for humans to comprehend directly. This lack of transparency means that even when a model achieves high performance, understanding \textit{why} it performs well or \textit{how} it arrives at its conclusions can be an arduous task, undermining trust and hindering deeper scientific inquiry \cite{ribeiro2016why, miller2019explanation}.

This \textit{black box} characteristic poses a particular impediment when these models are applied to Neuroscience, where the primary objective extends beyond mere replication of brain-like outputs to a fundamental understanding of the underlying neural mechanisms \cite{richards2019deep, savage2019how}. If a Deep Learning model accurately predicts neural responses in the visual cortex \cite{yamins2016using} or simulates language processing \cite{caucheteux2022brains} but its internal strategies remain inscrutable, it offers limited insight into how the biological brain actually performs these tasks. The aspiration of cognitive computational Neuroscience is to develop models that are \textit{not only predictive} but also \textit{explanatory}, providing testable hypotheses about brain function \cite{kriegeskorte2018cognitive}. A model that functions as an uninterpretable oracle, however accurate, falls short of this goal. It may demonstrate that a certain mapping from stimulus to neural activity or behavior is learnable, but it fails to reveal the computational principles or representational transformations that the brain itself might employ. Consequently, the scientific value for reverse-engineering brain mechanisms is obscured, as it becomes challenging to validate whether the model's learned solutions are neurobiologically plausible or to derive new insights into the brain's algorithms. The critical need, therefore, is to develop approaches that can illuminate these internal workings, transforming powerful predictive tools into genuinely explanatory frameworks for understanding the brain.

\section{A Taxonomy of Explainable AI Methods}\label{sec:XAI-tax}

To address the opacity of Deep Learning models, the field of \textbf{Explainable AI} (XAI) has emerged, offering a diverse variety of methods to take a look inside the \textit{black box} \cite{doshi2017towards, miller2019explanation}. A primary distinction in XAI methodologies lies between \textit{post-hoc interpretability techniques}, which are applied to already trained models, and \textit{intrinsically explainable models}, which are designed for transparency from the ground up. While some models like linear regression or decision trees are inherently interpretable due to their simple structure, the complex, highly non-linear nature of deep neural networks means they seldom fall into this category. Consequently, much of XAI research focuses on post-hoc methods to analyze these powerful but opaque systems \cite{doshi2017towards}.

One prominent category of post-hoc XAI techniques comprises \textbf{Feature Attribution Methods}. These methods aim to identify which parts of the input data a model deems most important when making a specific prediction.
\begin{itemize}
    \item \textbf{Saliency Maps} are a common feature attribution technique, particularly in computer vision. They generate heatmaps that highlight the pixels in an input image that most significantly influence the model's output for a given class \cite{fong2017interpretable}. Typically, these are computed by examining the gradient of the output prediction score with respect to the input pixel values. For neuroscientists, saliency maps can offer a visual hypothesis about which features a model, trained to mimic a sensory processing task, might be using, analogous to how one might study receptive fields in biological neurons.
    \item \textbf{LIME (Local Interpretable Model-agnostic Explanations)} and \textbf{SHAP (SHapley Additive exPlanations)} offer more sophisticated approaches to feature attribution. LIME operates by training a simpler, interpretable model (e.g., a linear model or a decision tree) to approximate the behavior of the complex black-box model in the local vicinity of a particular instance being explained \cite{ribeiro2016why}. This provides a local, understandable rationale for a specific prediction without requiring insight into the global structure of the original model. SHAP, on the other hand, draws from cooperative game theory, specifically Shapley values, to assign a unique contribution value to each feature for a given prediction, providing a unified and theoretically grounded framework for feature importance \cite{lundberg2017unified}. These methods are valuable because they can be applied to virtually any model (model-agnostic) and help study the reasoning behind individual predictions, which is crucial when trying to understand if an AI model's strategy bears any resemblance to neural computation.
\end{itemize}

Beyond attributing importance to input features, another class of XAI techniques focuses on \textbf{Model-based Interpretation}, aiming to understand the internal mechanisms and learned representations within the network itself.
\begin{itemize}
    \item \textbf{Analyzing Network Activations and Representations} involves directly inspecting the internal state of the network. This can include visualizing the activation patterns of individual neurons or entire layers in response to different inputs, or using techniques like feature visualization, which attempts to synthesize an input that maximally activates a specific neuron or feature detector within the network \cite{olah2018building}. Such methods allow researchers to probe what kinds of information are encoded at different stages of processing. For instance, the hierarchical features that CNNs learn when trained on image recognition, as we mentioned in Section~\ref{sec:2000s}~\cite{yamins2016using}. Other tools, like Network Dissection~\cite{bau2017network}, aim to quantify the interpretability of individual units by identifying the semantic concepts (e.g., 'door', 'sky') that they detect.

    \item \textbf{Ablation Studies} in artificial neural networks are directly inspired by lesion studies in Neuroscience. These methods involve systematically perturbing the model (for example, by desactivating specific artificial neurons, connections, or entire layers) and then observing the impact on the model's behavior or internal representations \cite{kriegeskorte2018cognitive}. If silencing a particular component significantly degrades performance on a specific task, it suggests that this component plays a crutial role in that computation. Such ``in silico lesioning'' can help to map functional roles to different parts of the network, providing testable hypotheses about the functional specialization of the different regions of the network.
\end{itemize}
These various XAI approaches are not mutually exclusive and are often used in combination to build a more comprehensive understanding of how complex AI models operate, thereby offering a richer set of tools for comparison with, and generation of hypotheses about, the brain.

\section{Explainable AI in Action: Bridging the Gap to Neuroscience}

The theoretical tools of XAI, as outlined in Section~\ref{sec:XAI-tax}, are not merely abstract concepts; they are actively being applied to bridge the gap between the high performance of AI models and a deeper neuroscientific understanding. By moving beyond simple performance metrics and towards model interpretability, researchers can start to reverse-engineer the complex computational processes of the brain.

One of the most fruitful applications of XAI in Neuroscience has been in the domain of \textbf{reverse-engineering sensory systems}. In the field of vision, for instance, while the initial success of CNNs was in object recognition, XAI techniques have allowed for a more nuanced analysis of their internal workings. Feature attribution methods and the analysis of network activations have revealed how these models learn to represent not just whole objects, but also fundamental visual properties like texture, shape, and motion. By comparing these learned representations with neural data from different stages of the visual cortex, researchers can generate and test specific hypotheses about how the brain deconstructs and processes visual scenes \cite{yamins2016using, kriegeskorte2018cognitive}. Similarly, in audition, Deep Learning models are being used to understand the neural coding of sound. Explainable models help to uncover how features like pitch, timbre, and rhythm are extracted and represented in the hierarchical layers of these networks, offering insights into the transformations that might be occurring in the biological auditory pathway \cite{richards2019deep}.

XAI methods are also central to the development of more sophisticated \textbf{encoding and decoding models} for brain signals. Encoding models aim to predict neural activity from given stimuli, while decoding models attempt the reverse: to infer mental states or perceived stimuli from neural recordings. Explainable models are crucial here, as they allow researchers to understand \textit{which} features of a stimulus drive neural responses (in encoding) or \textit{which} patterns of neural activity are most informative for a particular mental state (in decoding). For example, an explainable encoding model for the visual cortex might reveal not just that a certain neuron fires in response to a face, but that its firing is specifically driven by the presence of an eye or the curvature of a smile. This level of detail is essential for building a mechanistic understanding of neural representation and for developing more precise brain-computer interfaces \cite{schrimpf2020brain}.

Furthermore, XAI is beginning to shed light on \textbf{higher cognitive functions}. In the realm of language, researchers are now probing the internal representations of large language models (LLMs) like BERT and GPT to understand how they process and represent linguistic information. By comparing the activation patterns within these models to brain imaging data from humans engaged in language tasks, studies have found intriguing parallels between the models' representations and the activity in classical language centers of the brain, such as Broca's and Wernicke's areas \cite{caucheteux2022brains, toneva2019interpreting}. This comparative approach helps to formulate hypotheses about how the brain might implement computations for syntax, semantics, and context. In the domain of decision-making, explainable reinforcement learning (RL) models are providing insights into the neural circuits underlying reward, planning, and action selection. By analyzing the value functions and policies learned by RL agents, researchers can draw parallels to the dopaminergic systems and prefrontal cortex activity involved in goal-directed behavior, offering a computational framework for understanding both adaptive and maladaptive decision-making processes \cite{richards2019deep}. Through these applications, XAI is proving to be an indispensable tool, transforming AI models from black-box mimics into interpretable frameworks for generating and testing hypotheses about the brain.

\clearpage