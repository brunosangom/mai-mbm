\documentclass[11pt,a4paper]{article}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{nameref}
\usepackage[margin=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[style=numeric, sorting=none]{biblatex}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\hypersetup{
    colorlinks,
    linkcolor=black,  
    urlcolor=blue,
    citecolor=blue
}
\setlength{\marginparwidth}{2cm}
\usepackage{comment}
\usepackage{float}
\usepackage{booktabs}

\bibliography{references}
\author{Bruno S치nchez G칩mez}
\date{\today}


\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\Huge \bfseries MBM Essay 1: Human \par}
    \vspace{2cm}
    {\Large {\Huge Reverse-Engineering Brain Mechanisms through Explainable AI} \par}
    \vspace{8cm}
    {\large \textbf{Bruno S치nchez G칩mez} \par}
    \vfill
    {\large \today \par}
\end{titlepage}


\part{The Symbiotic Evolution of AI and Neuroscience: A Historical Perspective}

\section{Introduction}

The human brain is the most complex computational machine known to mankind. As such, philosophers and scientists have been fascinated by it for centuries, and have put great effort into trying to understand how its billions of neurons, working together, can create thoughts, perceptions, and consciousness.

This essay will look at the closely connected history of Artificial Intelligence (AI) and Neuroscience, two symbiotic fields that often take inspiration from each other. From the earliest attempts to create computational models based on neural processes to the cutting-edge use of explainable neural networks to better our understanding of the brain's complex mechanisms, progress in one domain has consistently spurred innovation in the other. We will follow this shared evolution, looking at how ideas from Neuroscience have influenced AI designs, and how AI now gives us new tools and ways to think about reverse-engineering the brain. The discussion will culminate in a review of the current state-of-the-art in this field and a look towards future directions where this collaboration promises to yield even deeper understanding of both biological and artificial intelligence.

The essay is structured as follows: Part I will begin with a historical review, charting the dawn of AI and early neural models, the rise of connectionism, and the transformative impact of the Deep Learning revolution on Neuroscience. Part II will delve into the ``black box'' problem inherent in many complex AI models and the subsequent birth of Explainable AI (XAI), detailing various methodologies and their crucial applications in closing the gap between AI performance and neuroscientific understanding. Finally, Part III will assess the current frontiers, discussing generative models, the shift beyond supervised learning, the rise of ``Neuro-AI'', and the use of large language models to simulate thought processes, while also considering the challenges, limitations, and ethical considerations inherent in this rapidly evolving field.

\section{The Dawn of AI and Early Neural Models (1940s-1960s)}

The foundational period of AI, spanning from the 1940s to the 1960s, was marked by pioneering efforts to conceptualize and model neural processes mathematically. A seminal contribution during this era was the McCulloch-Pitts neuron, introduced in 1943 \cite{mcculloch1943logical}. This model presented a simplified, logical abstraction of a biological neuron, proposing that neurons could be understood as computational units performing logical operations. Although a simplification of true neuronal complexity, the McCulloch-Pitts neuron was a groundbreaking concept. It laid crucial groundwork for both the nascent field of AI, by suggesting that machines could, in principle, perform tasks analogous to human thought, and for computational Neuroscience, by providing a formal framework to begin modeling neural activity and networks.

Building on the idea of interconnected processing units, Donald Hebb, in his influential 1949 work, ``The Organization of Behavior'' \cite{hebb1949organization}, proposed a mechanism for learning in the brain. His famous postulate, often summarized as ``neurons that fire together, wire together,'' introduced the concept of synaptic plasticity. Hebbian learning suggested that the strength of a connection between two neurons increases when they are activated simultaneously. This principle was profoundly influential, offering a plausible biological basis for how learning and memory could arise from neural activity and directly inspiring early learning algorithms in artificial neural networks. It provided a dynamic element to the static connections of earlier models, suggesting how networks could adapt and learn from experience.

Furthering the development of learning machines, Frank Rosenblatt introduced the Perceptron in 1958 \cite{rosenblatt1958perceptron}. The Perceptron was a more concrete implementation of a learning neural network, capable of learning to classify patterns by adjusting its synaptic weights based on errors. This invention generated considerable excitement, as it demonstrated a machine that could learn from data and perform pattern recognition tasks, a key aspect of intelligence. The Perceptron's architecture and learning rule were seen as analogous to early models of sensory processing in the brain, particularly in how simple features might be detected and combined to recognize more complex stimuli. It represented a significant step towards building practical AI systems inspired by neural principles.

Despite these early successes and the initial optimism, the field soon encountered significant challenges. In 1969, Marvin Minsky and Seymour Papert published ``Perceptrons'' \cite{minsky1969perceptrons}, a detailed mathematical analysis that highlighted severe limitations of single-layer Perceptrons, most notably their inability to solve problems that were not linearly separable, such as the XOR problem. This critique, coupled with exaggerated claims and unmet expectations, led to a significant reduction in funding and interest in neural network research, a period often referred to as the first ``AI winter.'' However, the foundational ideas laid during these early decades were not entirely abandoned. They simmered beneath the surface, awaiting new algorithmic breakthroughs and computational power that would eventually lead to a resurgence and the development of more powerful, multi-layered neural networks in the decades to follow.

\section{Connectionism and the Rise of Parallel Distributed Processing (1980s)}

After the ``AI winter,'' the 1980s witnessed a significant resurgence of interest in neural networks, largely fueled by the emergence of connectionism and the concept of Parallel Distributed Processing (PDP). Central to this revival was the work of the PDP Research Group, particularly David Rumelhart, Geoffrey Hinton, and James McClelland. Their influential two-volume book, ``Parallel Distributed Processing: Explorations in the Microstructure of Cognition'' \cite{mcclelland1986parallel}, often referred to as the ``Connectionist Bible,'' provided a comprehensive theoretical framework and compelling demonstrations of neural network capabilities. This work emphasized how complex cognitive phenomena could emerge from the parallel interactions of many simple processing units, akin to neurons. Connectionist models proposed that information was not stored in specific locations but distributed across connections, and that learning occurred through the modification of these connection strengths. This period also saw the popularization of ideas related to the capabilities of multi-layer networks.

A critical breakthrough that enabled the practical training of these more complex, multi-layer networks was the popularization and refinement of the backpropagation algorithm, notably described by Rumelhart, Hinton, and Williams in 1986 \cite{rumelhart1986learning}. While the core ideas of backpropagation had been explored earlier by others, their work made it accessible and demonstrated its power. Backpropagation provided an efficient method for calculating the gradient of the error function with respect to the network's weights, allowing for the systematic adjustment of these weights to minimize error. This meant that networks with hidden layers, which were previously difficult to train, could now learn complex input-output mappings. The ability to train multi-layer networks was a game-changer, as these architectures could overcome the limitations of single-layer perceptrons identified by Minsky and Papert, enabling the modeling of non-linear relationships and, consequently, more sophisticated cognitive functions.

The advancements in network architectures and training algorithms led to a wave of early applications in cognitive science, where PDP models were used to simulate and offer insights into a variety of human cognitive processes. These early applications, while often simplified, were crucial in demonstrating the potential of neural networks as tools for understanding the mind and brain, bridging the gap between abstract computational principles and observable cognitive phenomena \cite{mcclelland1986parallel}.

\section{The Deep Learning Revolution and its Impact on Neuroscience (2000s-Present)}\label{sec:2000s}

The new millennium marked the beginning of a new era for AI, largely characterized by the rise of Deep Learning. This revolution, which has only gained more momentum since the early 2000s, has had a profound and reciprocal impact on Neuroscience. The convergence of several key factors led to what some have termed the ``unreasonable effectiveness'' of Deep Learning models \cite{lecun2015deep}.
\begin{itemize}
    \item Firstly, the availability of massive datasets, such as ImageNet, provided the rich training material necessary for complex models to learn intricate patterns.
    \item Secondly, the parallel development and widespread adoption of powerful Graphics Processing Units (GPUs) offered the computational horsepower required to train these data-hungry and computationally intensive deep neural networks in a feasible timeframe.
    \item This combination of big data and powerful hardware unlocked the potential of Deep Learning architectures, allowing them to achieve state-of-the-art performance on a wide range of tasks, from image recognition to natural language processing, and in doing so, provided new, powerful tools for neuroscientific inquiry \cite{richards2019deep}.
\end{itemize}

Among the most influential Deep Learning architectures for Neuroscience have been Convolutional Neural Networks (CNNs). Initially inspired by the hierarchical processing observed in the mammalian visual cortex, CNNs have demonstrated remarkable success in computer vision tasks. These networks typically consist of multiple layers, including: convolutional layers, that apply filters to input images to detect features; pooling layers, that reduce dimensionality; and fully connected layers, that perform classification. The hierarchical nature of CNNs, where early layers learn simple features (like edges and textures), and deeper layers learn more complex and abstract representations (like object parts or even whole objects), bears a striking resemblance to the processing stages in the primate visual pathway \cite{yamins2016using}. This architectural similarity has made CNNs invaluable tools for modeling the visual cortex. Neuroscientists have used them not only to predict neural responses to visual stimuli with unprecedented accuracy but also to generate hypotheses about how visual information is represented and transformed in the brain \cite{kriegeskorte2018cognitive, savage2019how}. By comparing the internal representations of CNNs trained on visual tasks with neural recordings from different stages of the visual system, researchers have found compelling correspondences, suggesting that these artificial models might capture fundamental principles of biological vision.

Similarly, Recurrent Neural Networks (RNNs) have provided powerful models for understanding how the brain processes sequential data, such as language or temporal patterns in sensory input. Unlike feedforward networks like CNNs, RNNs possess connections that loop back on themselves, allowing them to maintain an internal state or ``memory'' of past inputs. This characteristic makes them well-suited for tasks where context and temporal dependencies are crucial. The ability of RNNs, particularly variants like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) which can learn long-range dependencies, to capture complex temporal dynamics has made them instrumental in exploring the neural mechanisms underlying time-sensitive cognitive processes \cite{richards2019deep}.

\clearpage

\part{The Black Box Dilemma and the Rise of Explainable AI (XAI)}

\section{The ``Black Box'' Problem in Deep Learning}

The remarkable success of Deep Learning models in emulating complex cognitive functions \cite{lecun2015deep}, as explored in Section~\ref{sec:2000s}, brings with it a significant challenge: their inherent opacity. Deep neural networks, with their vast number of parameters and intricate, multi-layered architectures, often operate as ``black boxes'' \cite{doshi2017towards}. While their input-output behavior can be meticulously evaluated, the internal computations and learned representations that lead to a specific decision or prediction are frequently obscure and difficult for humans to comprehend directly. This lack of transparency means that even when a model achieves high performance, understanding \textit{why} it performs well or \textit{how} it arrives at its conclusions can be an arduous task, undermining trust and hindering deeper scientific inquiry \cite{ribeiro2016why, miller2019explanation}.

This ``black box'' characteristic poses a particular impediment when these models are applied to Neuroscience, where the primary objective extends beyond mere replication of brain-like outputs to a fundamental understanding of the underlying neural mechanisms \cite{richards2019deep, savage2019how}. If a Deep Learning model accurately predicts neural responses in the visual cortex \cite{yamins2016using} or simulates language processing \cite{caucheteux2022brains} but its internal strategies remain inscrutable, it offers limited insight into how the biological brain actually performs these tasks. The aspiration of cognitive computational Neuroscience is to develop models that are not only predictive but also explanatory, providing testable hypotheses about brain function \cite{kriegeskorte2018cognitive}. A model that functions as an uninterpretable oracle, however accurate, falls short of this goal. It may demonstrate that a certain mapping from stimulus to neural activity or behavior is learnable, but it fails to reveal the computational principles or representational transformations that the brain itself might employ. Consequently, the scientific value for reverse-engineering brain mechanisms is obscured, as it becomes challenging to validate whether the model's learned solutions are neurobiologically plausible or to derive new insights into the brain's algorithms. The critical need, therefore, is to develop approaches that can illuminate these internal workings, transforming powerful predictive tools into genuinely explanatory frameworks for understanding the brain.

\section{A Taxonomy of Explainable AI Methods}

To address the opacity of Deep Learning models, the field of Explainable AI (XAI) has emerged, offering a diverse variety of methods to take a look inside the ``black box'' \cite{doshi2017towards, miller2019explanation}. A primary distinction in XAI methodologies lies between \textit{post-hoc interpretability techniques}, which are applied to already trained models, and \textit{intrinsically explainable models}, which are designed for transparency from the ground up. While some models like linear regression or decision trees are inherently interpretable due to their simple structure, the complex, highly non-linear nature of deep neural networks means they seldom fall into this category. Consequently, much of XAI research focuses on post-hoc methods to analyze these powerful but opaque systems \cite{doshi2017towards}.

One prominent category of post-hoc XAI techniques comprises \textbf{Feature Attribution Methods}. These methods aim to identify which parts of the input data a model deems most important when making a specific prediction.
\begin{itemize}
    \item \textbf{Saliency Maps} are a common feature attribution technique, particularly in computer vision. They generate heatmaps that highlight the pixels in an input image that most significantly influence the model's output for a given class \cite{fong2017interpretable}. Typically, these are computed by examining the gradient of the output prediction score with respect to the input pixel values. For neuroscientists, saliency maps can offer a visual hypothesis about which features a model, trained to mimic a sensory processing task, might be using, analogous to how one might study receptive fields in biological neurons.
    \item \textbf{LIME (Local Interpretable Model-agnostic Explanations)} and \textbf{SHAP (SHapley Additive exPlanations)} offer more sophisticated approaches to feature attribution. LIME operates by training a simpler, interpretable model (e.g., a linear model or a decision tree) to approximate the behavior of the complex black-box model in the local vicinity of a particular instance being explained \cite{ribeiro2016why}. This provides a local, understandable rationale for a specific prediction without requiring insight into the global structure of the original model. SHAP, on the other hand, draws from cooperative game theory, specifically Shapley values, to assign a unique contribution value to each feature for a given prediction, providing a unified and theoretically grounded framework for feature importance \cite{lundberg2017unified}. These methods are valuable because they can be applied to virtually any model (model-agnostic) and help study the reasoning behind individual predictions, which is crucial when trying to understand if an AI model's strategy bears any resemblance to neural computation.
\end{itemize}

Beyond attributing importance to input features, another class of XAI techniques focuses on \textbf{Model-based Interpretation}, aiming to understand the internal mechanisms and learned representations within the network itself.
\begin{itemize}
    \item \textbf{Analyzing Network Activations and Representations} involves directly inspecting the internal state of the network. This can include visualizing the activation patterns of individual neurons or entire layers in response to different inputs, or using techniques like feature visualization, which attempts to synthesize an input that maximally activates a specific neuron or feature detector within the network \cite{olah2018building}. Such methods allow researchers to probe what kinds of information are encoded at different stages of processing. For instance, the hierarchical features that CNNs learn when trained on image recognition, as we mentioned in Section~\ref{sec:2000s}~\cite{yamins2016using}. Other tools, like Network Dissection~\cite{bau2017network}, aim to quantify the interpretability of individual units by identifying the semantic concepts (e.g., 'door', 'sky') that they detect.

    \item \textbf{Ablation Studies} in artificial neural networks are directly inspired by lesion studies in Neuroscience. These methods involve systematically perturbing the model (for example, by desactivating specific artificial neurons, connections, or entire layers) and then observing the impact on the model's behavior or internal representations \cite{kriegeskorte2018cognitive}. If silencing a particular component significantly degrades performance on a specific task, it suggests that this component plays a crutial role in that computation. Such ``in silico lesioning'' can help to map functional roles to different parts of the network, providing testable hypotheses about the functional specialization of the different regions of the network.
\end{itemize}
These various XAI approaches are not mutually exclusive and are often used in combination to build a more comprehensive understanding of how complex AI models operate, thereby offering a richer set of tools for comparison with, and generation of hypotheses about, the brain.

\section{Explainable AI in Action: Bridging the Gap to Neuroscience}
\begin{itemize}
    \item \textbf{Reverse-Engineering Sensory Systems:}
        \begin{itemize}
            \item \textbf{Vision:} Beyond object recognition, how XAI helps us understand how CNNs represent texture, shape, and motion, and how this compares to neural data from different visual areas.
            \item \textbf{Audition:} Using Deep Learning models to understand the neural coding of sound, from the cochlea to the auditory cortex.
        \end{itemize}
    \item \textbf{Decoding and Encoding Models:} Using explainable models to predict neural activity from stimuli (encoding) and to decode mental states or perceived stimuli from neural recordings (decoding).
    \item \textbf{Understanding Higher Cognitive Functions:}
        \begin{itemize}
            \item \textbf{Language:} Analyzing the representations within large language models (LLMs) like BERT and GPT, and comparing them to the brain's language processing centers (e.g., Broca's and Wernicke's areas).
            \item \textbf{Decision-Making and Reinforcement Learning:} How explainable reinforcement learning models can shed light on the neural circuits involved in reward, planning, and action selection.
        \end{itemize}
\end{itemize}

\clearpage

\part{The State of the Art and the Future of Brain-Inspired AI}

\section{Current Frontiers in Explainable Neural Networks for Neuroscience}
\begin{itemize}
    \item \textbf{Generative Models and ``In Silico'' Experiments:} Using generative adversarial networks (GANs) and other generative models to create stimuli that maximally activate specific neurons or brain regions, allowing for more targeted experiments.
    \item \textbf{Beyond Supervised Learning:} The role of self-supervised and unsupervised learning in creating models that learn more brain-like representations without requiring massive labeled datasets.
    \item \textbf{The Rise of ``Neuro-AI'':} The growing field of research that explicitly aims to build AI systems based on principles from Neuroscience, creating a virtuous cycle of discovery.
    \item \textbf{Thinking LLMs and Simulating Thought Processes:} Discussing the emerging use of large language models to model and understand human-like reasoning, planning, and problem-solving.
\end{itemize}

\section{Challenges, Limitations, and Ethical Considerations}
\begin{itemize}
    \item \textbf{The ``Simile'' vs. ``Model'' Distinction:} Emphasize that even the most brain-like ANNs are still simplifications. Discuss the key biological details they often omit (e.g., dendritic computation, neuromodulation).
    \item \textbf{The Dangers of Over-interpretation:} The risk of drawing premature or overly simplistic conclusions about the brain based on analogies with AI models.
    \item \textbf{Data Privacy and Neuromarketing:} Briefly touch on the ethical implications of being able to decode brain states with increasing accuracy.
\end{itemize}

\section{Conclusion: The Future of a Fruitful Partnership}
\begin{itemize}
    \item \textbf{Recap of the Main Arguments:} Summarize the historical co-evolution and the current state of synergy between AI and Neuroscience.
    \item \textbf{Future Outlook:} Project how this interdisciplinary collaboration will continue to unravel the complexities of the brain and, in turn, inspire more general and capable artificial intelligence. The ultimate goal: a unified theory of intelligence, both biological and artificial.
    \item \textbf{Final Thought-Provoking Statement:} Reiterate the profound potential of this research to not only advance science but also to fundamentally alter our understanding of ourselves.
\end{itemize}

\clearpage
\nocite{*}
\printbibliography%[title=References]

\end{document}